model:
  class_path: modules.FastGANModule
  init_args:
    im_size: 256
    nc: 16
    ndf: 64
    ngf: 64
    nz: 256
    nlr: 0.0002
    nbeta1: 0.5
    nbeta2: 0.999
    log_images_on_step_n: 50
    val_check_interval: 50
    pred_global_max: [0.6203158, 0.6172642, 0.46794897, 0.4325111, 0.4996644, 0.61997396,
                  0.7382196, 0.86097705, 0.88304037, 0.9397393, 1.1892519, 1.5035477,
                  1.4947973, 1.4737314, 1.6318618, 1.7226081]
    pred_global_min: [0.00028473, 0.0043945, 0.00149752, 0.00167517, 0.00190101, 0.0028114,
                  0.00394378, 0.00488099, 0.00257091, 0.00215704, 0.00797662, 0.01205248,
                  0.01310135, 0.01476806, 0.01932094, 0.02020744]
seed_everything: 114514

ckpt_path: logs/hypersynth/p9dz8o09/checkpoints/step=0-val_MIFID=104.0265.ckpt # dysplastic_nevi
# ckpt_path: logs/hypersynth/ic81nq11/checkpoints/step=0-val_MIFID=94.2868_fixed.ckpt # melanoma

trainer:
  # precision: 16-mixed
  max_epochs: -1
  log_every_n_steps: 20
  max_steps: 100_000
  check_val_every_n_epoch:

  logger:
    # https://lightning.ai/docs/pytorch/latest/api/lightning.pytorch.loggers.wandb.html#module-lightning.pytorch.loggers.wandb
    class_path: WandbLogger
    init_args:
      save_dir: logs
      entity: k298976-unicamp
      project: hypersynth
      # adding the run name manually will override the automatic naming
      # name: densenet201_hsi_classifier
      log_model: false
      notes: "Write something interesting about this experiment here."
      
  # Callbacks https://lightning.ai/docs/pytorch/latest/extensions/callbacks.html
  callbacks:
    # - class_path: callbacks.ImageSavingCallback
    #   init_args:
    #     fixed_noise_shape: [8, 256]
    #     saved_image_folder: logs/images
    #     nc: 16
    # https://lightning.ai/docs/pytorch/latest/api/lightning.pytorch.callbacks.ModelCheckpoint.html#lightning.pytorch.callbacks.ModelCheckpoint
    - class_path: callbacks.FixedModelCheckpoint
      init_args:
        filename: step={global_step}-val_MIFID={val/MIFID:.4f}
        monitor: val/MIFID
        verbose: true
        save_last: true
        save_top_k: 1
        mode: min
        auto_insert_metric_name: false
        every_n_train_steps: 200
    # # # https://lightning.ai/docs/pytorch/latest/api/lightning.pytorch.callbacks.EarlyStopping.html#lightning.pytorch.callbacks.EarlyStopping
    # - class_path: EarlyStopping
    #   init_args:
    #     monitor: val/SAM
    #     min_delta: 0.01
    #     patience: 200
    #     verbose: true
    #     mode: min
    #     strict: true
    #     check_on_train_epoch_end: False
    # https://lightning.ai/docs/pytorch/latest/api/lightning.pytorch.callbacks.LearningRateMonitor.html#lightning.pytorch.callbacks.LearningRateMonitor
    - class_path: LearningRateMonitor
      init_args:
        logging_interval: epoch
